{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "- baseline estimator\n",
    "- SVD\n",
    "- SVD + bias\n",
    "- SVD + bias + attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    # 读取train.txt格式的数据，返回字典\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline().strip()\n",
    "            if not line:  # EOF\n",
    "                break\n",
    "            # 读取user_id和rate_num\n",
    "            user_id, rate_num = line.split('|')\n",
    "            rate_num = int(rate_num)\n",
    "            user_id = int(user_id)\n",
    "            # 读取用户的评分数据\n",
    "            rate_data = {}\n",
    "            for i in range(rate_num):\n",
    "                item_id, score = f.readline().strip().split()\n",
    "                item_id = int(item_id)\n",
    "                score = int(score)\n",
    "                rate_data[item_id] = score\n",
    "            # 保存该用户的数据\n",
    "            data[user_id] = rate_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data): 19835\n"
     ]
    }
   ],
   "source": [
    "train_path =\"data/train_data.txt\"\n",
    "train_data = read_data(train_path)\n",
    "print(\"len(train_data):\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(valid_data): 19835\n"
     ]
    }
   ],
   "source": [
    "valid_path =\"data/validate_data.txt\"\n",
    "valid_data = read_data(valid_path)\n",
    "print(\"len(valid_data):\", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### μ : overall mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算全局平均分\n",
    "def cal_global_avg(data):\n",
    "    sum_score = 0\n",
    "    sum_num = 0\n",
    "    for user_id, rate_data in data.items():\n",
    "        sum_score += sum(rate_data.values())\n",
    "        sum_num += len(rate_data)\n",
    "    return sum_score / sum_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_avg: 49.47062753677192\n"
     ]
    }
   ],
   "source": [
    "global_avg = cal_global_avg(train_data)\n",
    "print(\"global_avg:\", global_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_x : rating deviation of user x (ave.rating of user x - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个用户的平均评分，用户偏差\n",
    "def cal_user_bias(data, average_score):\n",
    "    # 每个用户的平均评分\n",
    "    user_average_score = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        total_score = 0\n",
    "        for score in rate_data.values():\n",
    "            total_score += score\n",
    "        user_average_score[user_id] = total_score / len(rate_data)\n",
    "    # 每个用户与全局平均评分的偏差\n",
    "    user_bias = {}\n",
    "    for user_id, u_ave_score in user_average_score.items():\n",
    "        user_bias[user_id] = u_ave_score - average_score\n",
    "    # 最小偏差，最大偏差，平均偏差\n",
    "    max_bias = max(user_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(user_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in user_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(user_bias)\n",
    "    return user_average_score, user_bias, max_bias, min_bias, average_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (547, 50.52937246322808)\n",
      "min_bias: (413, -49.47062753677192)\n",
      "average_bias: 20.37407831847788\n"
     ]
    }
   ],
   "source": [
    "user_average_score, user_bias, max_bias, min_bias, average_bias = cal_user_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_i : rating deviation of item i (ave.rating of item i - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个物品的平均评分，物品偏差\n",
    "def cal_item_bias(data, average_score):\n",
    "    # 统计物品得分\n",
    "    item_scores = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            if item_id in item_scores:\n",
    "                item_scores[item_id].append(score)\n",
    "            else:\n",
    "                item_scores[item_id] = [score]\n",
    "    # 计算物品平均得分\n",
    "    item_average_score = {}\n",
    "    for item_id, scores in item_scores.items():\n",
    "        item_average_score[item_id] = sum(scores) / len(scores)\n",
    "    # 计算物品偏差\n",
    "    item_bias = {}\n",
    "    for item_id, i_ave_score in item_average_score.items():\n",
    "        item_bias[item_id] = i_ave_score - average_score\n",
    "    # 最大偏差，最小偏差，平均偏差\n",
    "    max_bias = max(item_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(item_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in item_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(item_bias)\n",
    "    return item_average_score, item_bias, max_bias, min_bias, average_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (210761, 50.52937246322808)\n",
      "min_bias: (211658, -49.47062753677192)\n",
      "average_bias: -5.654850049099554\n"
     ]
    }
   ],
   "source": [
    "item_average_score, item_bias, max_bias, min_bias, average_bias = cal_item_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineEstimator:\n",
    "    def __init__(self, global_avg, user_bias, item_bias):\n",
    "        self.global_avg = global_avg\n",
    "        self.user_bias = user_bias\n",
    "        self.item_bias = item_bias\n",
    "        self.baseline_estimator = {}\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        for user_id, rate_data in train_data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                self.baseline_estimator[(user_id, item_id)] = self.global_avg + self.user_bias[user_id] + self.item_bias[item_id]\n",
    "\n",
    "    def save_model(self, path=\"models/baseline_estimator.pkl\"):\n",
    "        # 保存自身模型\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        return self.baseline_estimator.get((user_id, item_id), self.global_avg)  # 不存在则使用global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.47062753677192\n"
     ]
    }
   ],
   "source": [
    "estimator = BaselineEstimator(global_avg, user_bias, item_bias)\n",
    "estimator.fit(train_data)\n",
    "estimator.save_model()  # 保存模型\n",
    "print(estimator.predict(0,0))  # 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_data: 5\n"
     ]
    }
   ],
   "source": [
    "baseline_data = {\n",
    "    \"global_avg\": global_avg,\n",
    "    \"user_average_score\": user_average_score,\n",
    "    \"user_bias\": user_bias,\n",
    "    \"item_average_score\": item_average_score,\n",
    "    \"item_bias\": item_bias\n",
    "}\n",
    "\n",
    "with open(\"models/baseline_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_data, f)\n",
    "\n",
    "print(\"baseline_data:\", len(baseline_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(data, model):\n",
    "    rmse, count = 0.0, 0\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            predict = model.predict(user_id, item_id)\n",
    "            rmse += (predict - score) ** 2\n",
    "            count += 1\n",
    "    rmse = np.sqrt((rmse / count))\n",
    "    return rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_rmse: 28.927723508286146\n",
      "len(valid_data): 19835\n",
      "baseline_rmse: 38.20913906323089\n"
     ]
    }
   ],
   "source": [
    "baseline_rmse = RMSE(train_data, estimator)\n",
    "print(\"baseline_rmse:\", baseline_rmse)\n",
    "\n",
    "baseline_rmse = RMSE(valid_data, estimator)\n",
    "print(\"baseline_rmse:\", baseline_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试将分数压缩到0-10之间(没啥用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data):\n",
    "    \"\"\"\n",
    "    压缩数据\n",
    "    Args:\n",
    "        data:数据\n",
    "    Returns:\n",
    "        compressed_data:压缩后的数据\n",
    "    \"\"\"\n",
    "    compressed_data={}\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            if user_id not in compressed_data:\n",
    "                compressed_data[user_id] = {}\n",
    "            compressed_data[user_id][item_id] = (float)(score/10)\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(compress_train_data): 19835\n",
      "len(compress_valid_data): 19835\n"
     ]
    }
   ],
   "source": [
    "compress_train_data = compress_data(train_data)\n",
    "print(\"len(compress_train_data):\", len(compress_train_data))\n",
    "compress_valid_data = compress_data(valid_data)\n",
    "print(\"len(compress_valid_data):\", len(compress_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_avg: 4.947062753677194\n",
      "max_bias: (547, 5.052937246322806)\n",
      "min_bias: (413, -4.947062753677194)\n",
      "average_bias: 2.037407831847696\n",
      "max_bias: (210761, 5.052937246322806)\n",
      "min_bias: (211658, -4.947062753677194)\n",
      "average_bias: -0.5654850049047325\n"
     ]
    }
   ],
   "source": [
    "global_avg = cal_global_avg(compress_train_data)\n",
    "print(\"global_avg:\", global_avg)\n",
    "\n",
    "user_average_score, user_bias, max_bias, min_bias, average_bias = cal_user_bias(compress_train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)\n",
    "\n",
    "item_average_score, item_bias, max_bias, min_bias, average_bias = cal_item_bias(compress_train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = {}\n",
    "with open(\"models/baseline_data.pkl\", \"rb\") as f:\n",
    "    baseline_data = pickle.load(f)\n",
    "# baseline_data[\"global_avg\"] = global_avg\n",
    "# baseline_data[\"user_bias\"] = user_bias\n",
    "# baseline_data[\"item_bias\"] = item_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(self, baseline_data, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2, \n",
    "                 lambda_bx = 1e-2, lambda_bi = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            baseline_data: dict, baseline数据\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "            lambda_bx: float, 正则化参数\n",
    "            lambda_bi: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        self.lambda_bx = lambda_bx\n",
    "        self.lambda_bi = lambda_bi\n",
    "        # 用户与物品偏置\n",
    "        self.global_avg = baseline_data[\"global_avg\"]\n",
    "        self.bx = baseline_data[\"user_bias\"]\n",
    "        self.bi = baseline_data[\"item_bias\"]\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        if user_id in self.bx.keys():\n",
    "            bx = self.bx[user_id]\n",
    "        else:\n",
    "            bx = 0\n",
    "        if item_id in self.bi.keys():\n",
    "            bi = self.bi[item_id]\n",
    "        else:\n",
    "            bi = 0\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        return self.global_avg + bx + bi + np.dot(p, q)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        loss += self.lambda_bx * np.linalg.norm(list(self.bx.values())) ** 2\n",
    "        loss += self.lambda_bi * np.linalg.norm(list(self.bi.values())) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    bx = self.bx[user_id]\n",
    "                    bi = self.bi[item_id]\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.bx[user_id] += lr * (error - self.lambda_bx * bx)\n",
    "                    self.bi[item_id] += lr * (error - self.lambda_bi * bi)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.9\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_model = SVD(baseline_data, factor=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [01:01<00:00, 323.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=28.630262387438922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [01:00<00:00, 325.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=28.541973399815507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [01:00<00:00, 327.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=28.53460224438406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [01:01<00:00, 322.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=28.55881790681439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [01:00<00:00, 326.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=28.597379083366164\n"
     ]
    }
   ],
   "source": [
    "SVD_model.train(5, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  17.307197138918582\n",
      "valid RMSE:  28.52235204416675\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_50factor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  16.062878369561574\n",
      "valid RMSE:  28.68405924929402\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/SVD_50factor.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "train_RMSE = RMSE(train_data, model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svd models' results, its parameters and the time it takes to train for one epoch are as follows:\n",
    "- n-factor(500), train RMSE(12.93), valid RMSE(27.95), time(180s)\n",
    "- n-factor(100), train RMSE(13.35), valid RMSE(28.75), time(?s)\n",
    "- n-factor(50), train RMSE(16.06), valid RMSE(28.52), time(60s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the attributes of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_attribute:\n",
    "    def __init__(self, baseline_data, similar_nodes, k = 3, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2, \n",
    "                 lambda_bx = 1e-2, lambda_bi = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            baseline_data: dict, baseline数据\n",
    "            similar_nodes: dict, 每个节点的相似节点\n",
    "            k: int, 使用的相似节点个数\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "            lambda_bx: float, 正则化参数\n",
    "            lambda_bi: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        self.lambda_bx = lambda_bx\n",
    "        self.lambda_bi = lambda_bi\n",
    "        # 用户与物品偏置\n",
    "        self.global_avg = baseline_data[\"global_avg\"]\n",
    "        self.bx = baseline_data[\"user_bias\"]\n",
    "        self.bi = baseline_data[\"item_bias\"]\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "        # 相似节点\n",
    "        self.similar_nodes = similar_nodes\n",
    "        self.k = k\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        if user_id in self.bx.keys():\n",
    "            bx = self.bx[user_id]\n",
    "        else:\n",
    "            bx = 0\n",
    "        if item_id in self.bi.keys():\n",
    "            bi = self.bi[item_id]\n",
    "        else:\n",
    "            bi = 0\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        # 直接得分由SVD模型得到\n",
    "        direct_score = self.global_avg + bx + bi + np.dot(p, q)\n",
    "        # indrect_score由相似节点得分平均得到\n",
    "        indirect_score, count = 0, 0\n",
    "        if item_id in self.similar_nodes:\n",
    "            for node_id in self.similar_nodes[item_id]:\n",
    "                temp_q = self.Q[:, node_id]\n",
    "                indirect_score += np.dot(p, temp_q)\n",
    "                count += 1\n",
    "                if count == self.k:\n",
    "                    break\n",
    "        if count == 0:\n",
    "            return direct_score\n",
    "        return direct_score * 0.6 + (indirect_score / count) * 0.4\n",
    "\n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        loss += self.lambda_bx * np.linalg.norm(list(self.bx.values())) ** 2\n",
    "        loss += self.lambda_bi * np.linalg.norm(list(self.bi.values())) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    bx = self.bx[user_id]\n",
    "                    bi = self.bi[item_id]\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.bx[user_id] += lr * (error - self.lambda_bx * bx)\n",
    "                    self.bi[item_id] += lr * (error - self.lambda_bi * bi)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(similar_nodes): 507172\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/similar_nodes.pkl\", \"rb\") as f:\n",
    "    similar_nodes = pickle.load(f)\n",
    "print(\"len(similar_nodes):\", len(similar_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_attribute_model = SVD_attribute(baseline_data, similar_nodes, k = 5, factor=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [01:47<00:00, 184.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=27.66431630504323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [01:43<00:00, 191.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=27.493125325653388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [01:42<00:00, 193.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=27.43568951280574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [01:42<00:00, 193.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=27.423526048537546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [01:37<00:00, 203.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=27.43480295694844\n"
     ]
    }
   ],
   "source": [
    "SVD_attribute_model.train(5, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  17.44006186078717\n",
      "valid RMSE:  27.35585258687036\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_attribute_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_attribute_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svd with attributes, its results are as follows:\n",
    "- n-factor(50), k(3), train RMSE(19.25), valid RMSE(27.65), time(100s), portion(8:2)\n",
    "- n-factor(50), k(5), train RMSE(17.27), valid RMSE(27.76), time(100s), portion(8:2)\n",
    "- n-factor(50), k(5), train RMSE(17.44), valid RMSE(27.35), time(100s), portion(6:4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_attribute_50_5_64.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_attribute_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test basic SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_basic:\n",
    "    def __init__(self, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        return np.dot(p, q)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [00:52<00:00, 379.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=40.002484993578314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [00:48<00:00, 406.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=33.767823748409384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [00:48<00:00, 407.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=31.969428124884836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [00:48<00:00, 406.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=31.011639914824503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [00:49<00:00, 401.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=30.472118593829926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 19835/19835 [00:49<00:00, 402.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished: validate loss=30.170215252608596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 19835/19835 [00:52<00:00, 381.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished: validate loss=30.00074837662762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 19835/19835 [00:49<00:00, 402.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished: validate loss=29.901940603063125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 19835/19835 [00:47<00:00, 413.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished: validate loss=29.841684392178344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 19835/19835 [00:48<00:00, 410.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished: validate loss=29.804003014157054\n"
     ]
    }
   ],
   "source": [
    "SVD_basic_model = SVD_basic(factor=50)\n",
    "SVD_basic_model.train(10, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  18.45212773434878\n",
      "valid RMSE:  29.802801955106954\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_basic_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_basic_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_basic_50factor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_basic_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
