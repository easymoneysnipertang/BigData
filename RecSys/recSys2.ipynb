{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "- baseline estimator\n",
    "- SVD\n",
    "- SVD + bias\n",
    "- SVD + bias + attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    # 读取train.txt格式的数据，返回字典\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline().strip()\n",
    "            if not line:  # EOF\n",
    "                break\n",
    "            # 读取user_id和rate_num\n",
    "            user_id, rate_num = line.split('|')\n",
    "            rate_num = int(rate_num)\n",
    "            user_id = int(user_id)\n",
    "            # 读取用户的评分数据\n",
    "            rate_data = {}\n",
    "            for i in range(rate_num):\n",
    "                item_id, score = f.readline().strip().split()\n",
    "                item_id = int(item_id)\n",
    "                score = int(score)\n",
    "                rate_data[item_id] = score\n",
    "            # 保存该用户的数据\n",
    "            data[user_id] = rate_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data): 19835\n"
     ]
    }
   ],
   "source": [
    "train_path =\"data/train_data.txt\"\n",
    "train_data = read_data(train_path)\n",
    "print(\"len(train_data):\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(valid_data): 19835\n"
     ]
    }
   ],
   "source": [
    "valid_path =\"data/validate_data.txt\"\n",
    "valid_data = read_data(valid_path)\n",
    "print(\"len(valid_data):\", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### μ : overall mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算全局平均分\n",
    "def cal_global_avg(data):\n",
    "    sum_score = 0\n",
    "    sum_num = 0\n",
    "    for user_id, rate_data in data.items():\n",
    "        sum_score += sum(rate_data.values())\n",
    "        sum_num += len(rate_data)\n",
    "    return sum_score / sum_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_avg: 49.47062753677192\n"
     ]
    }
   ],
   "source": [
    "global_avg = cal_global_avg(train_data)\n",
    "print(\"global_avg:\", global_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_x : rating deviation of user x (ave.rating of user x - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个用户的平均评分，用户偏差\n",
    "def cal_user_bias(data, average_score):\n",
    "    # 每个用户的平均评分\n",
    "    user_average_score = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        total_score = 0\n",
    "        for score in rate_data.values():\n",
    "            total_score += score\n",
    "        user_average_score[user_id] = total_score / len(rate_data)\n",
    "    # 每个用户与全局平均评分的偏差\n",
    "    user_bias = {}\n",
    "    for user_id, u_ave_score in user_average_score.items():\n",
    "        user_bias[user_id] = u_ave_score - average_score\n",
    "    # 最小偏差，最大偏差，平均偏差\n",
    "    max_bias = max(user_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(user_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in user_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(user_bias)\n",
    "    return user_average_score, user_bias, max_bias, min_bias, average_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (547, 50.52937246322808)\n",
      "min_bias: (413, -49.47062753677192)\n",
      "average_bias: 20.37407831847788\n"
     ]
    }
   ],
   "source": [
    "user_average_score, user_bias, max_bias, min_bias, average_bias = cal_user_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_i : rating deviation of item i (ave.rating of item i - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个物品的平均评分，物品偏差\n",
    "def cal_item_bias(data, average_score):\n",
    "    # 统计物品得分\n",
    "    item_scores = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            if item_id in item_scores:\n",
    "                item_scores[item_id].append(score)\n",
    "            else:\n",
    "                item_scores[item_id] = [score]\n",
    "    # 计算物品平均得分\n",
    "    item_average_score = {}\n",
    "    for item_id, scores in item_scores.items():\n",
    "        item_average_score[item_id] = sum(scores) / len(scores)\n",
    "    # 计算物品偏差\n",
    "    item_bias = {}\n",
    "    for item_id, i_ave_score in item_average_score.items():\n",
    "        item_bias[item_id] = i_ave_score - average_score\n",
    "    # 最大偏差，最小偏差，平均偏差\n",
    "    max_bias = max(item_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(item_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in item_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(item_bias)\n",
    "    return item_average_score, item_bias, max_bias, min_bias, average_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (210761, 50.52937246322808)\n",
      "min_bias: (211658, -49.47062753677192)\n",
      "average_bias: -5.654850049099554\n"
     ]
    }
   ],
   "source": [
    "item_average_score, item_bias, max_bias, min_bias, average_bias = cal_item_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineEstimator:\n",
    "    def __init__(self, global_avg, user_bias, item_bias):\n",
    "        self.global_avg = global_avg\n",
    "        self.user_bias = user_bias\n",
    "        self.item_bias = item_bias\n",
    "        self.baseline_estimator = {}\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        for user_id, rate_data in train_data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                self.baseline_estimator[(user_id, item_id)] = self.global_avg + self.user_bias[user_id] + self.item_bias[item_id]\n",
    "\n",
    "    def save_model(self, path=\"models/baseline_estimator.pkl\"):\n",
    "        # 保存自身模型\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        return self.baseline_estimator.get((user_id, item_id), self.global_avg)  # 不存在则使用global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.47062753677192\n"
     ]
    }
   ],
   "source": [
    "estimator = BaselineEstimator(global_avg, user_bias, item_bias)\n",
    "estimator.fit(train_data)\n",
    "estimator.save_model()  # 保存模型\n",
    "print(estimator.predict(0,0))  # 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_data: 5\n"
     ]
    }
   ],
   "source": [
    "baseline_data = {\n",
    "    \"global_avg\": global_avg,\n",
    "    \"user_average_score\": user_average_score,\n",
    "    \"user_bias\": user_bias,\n",
    "    \"item_average_score\": item_average_score,\n",
    "    \"item_bias\": item_bias\n",
    "}\n",
    "\n",
    "with open(\"models/baseline_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_data, f)\n",
    "\n",
    "print(\"baseline_data:\", len(baseline_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(data, model):\n",
    "    rmse, count = 0.0, 0\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            predict = model.predict(user_id, item_id)\n",
    "            rmse += (predict - score) ** 2\n",
    "            count += 1\n",
    "    rmse = np.sqrt((rmse / count))\n",
    "    return rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_rmse: 28.927723508286146\n",
      "baseline_rmse: 38.20913906323089\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/baseline_estimator.pkl\", \"rb\") as f:\n",
    "    estimator = pickle.load(f)\n",
    "\n",
    "baseline_rmse = RMSE(train_data, estimator)\n",
    "print(\"baseline_rmse:\", baseline_rmse)\n",
    "\n",
    "baseline_rmse = RMSE(valid_data, estimator)\n",
    "print(\"baseline_rmse:\", baseline_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "various SVD models with 50 latent factors:\n",
    "- basic SVD\n",
    "  - time of 1 epoch: 50s\n",
    "  - train RMSE:  17.82015769020438\n",
    "  - valid RMSE:  29.4390823320279\n",
    "- SVD + bias\n",
    "  - time of 1 epoch: 75s\n",
    "  - train RMSE:  16.57224154523797\n",
    "  - valid RMSE:  27.77936363893447\n",
    "- SVD + bias + attributes (k=3 portion=7:3)\n",
    "  - time of 1 epoch: 100s\n",
    "  - train RMSE:  16.652290000697285\n",
    "  - valid RMSE:  26.983873535843795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试将分数压缩到0-10之间(没啥用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data):\n",
    "    \"\"\"\n",
    "    压缩数据\n",
    "    Args:\n",
    "        data:数据\n",
    "    Returns:\n",
    "        compressed_data:压缩后的数据\n",
    "    \"\"\"\n",
    "    compressed_data={}\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            if user_id not in compressed_data:\n",
    "                compressed_data[user_id] = {}\n",
    "            compressed_data[user_id][item_id] = (float)(score/10)\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(compress_train_data): 19835\n",
      "len(compress_valid_data): 19835\n"
     ]
    }
   ],
   "source": [
    "compress_train_data = compress_data(train_data)\n",
    "print(\"len(compress_train_data):\", len(compress_train_data))\n",
    "compress_valid_data = compress_data(valid_data)\n",
    "print(\"len(compress_valid_data):\", len(compress_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_avg: 4.947062753677194\n",
      "max_bias: (547, 5.052937246322806)\n",
      "min_bias: (413, -4.947062753677194)\n",
      "average_bias: 2.037407831847696\n",
      "max_bias: (210761, 5.052937246322806)\n",
      "min_bias: (211658, -4.947062753677194)\n",
      "average_bias: -0.5654850049047325\n"
     ]
    }
   ],
   "source": [
    "global_avg = cal_global_avg(compress_train_data)\n",
    "print(\"global_avg:\", global_avg)\n",
    "\n",
    "user_average_score, user_bias, max_bias, min_bias, average_bias = cal_user_bias(compress_train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)\n",
    "\n",
    "item_average_score, item_bias, max_bias, min_bias, average_bias = cal_item_bias(compress_train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = {}\n",
    "with open(\"models/baseline_data.pkl\", \"rb\") as f:\n",
    "    baseline_data = pickle.load(f)\n",
    "# baseline_data[\"global_avg\"] = global_avg\n",
    "# baseline_data[\"user_bias\"] = user_bias\n",
    "# baseline_data[\"item_bias\"] = item_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(self, baseline_data, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2, \n",
    "                 lambda_bx = 1e-2, lambda_bi = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            baseline_data: dict, baseline数据\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "            lambda_bx: float, 正则化参数\n",
    "            lambda_bi: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        self.lambda_bx = lambda_bx\n",
    "        self.lambda_bi = lambda_bi\n",
    "        # 用户与物品偏置\n",
    "        self.global_avg = baseline_data[\"global_avg\"]\n",
    "        self.bx = baseline_data[\"user_bias\"]\n",
    "        self.bi = baseline_data[\"item_bias\"]\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        if user_id in self.bx.keys():\n",
    "            bx = self.bx[user_id]\n",
    "        else:\n",
    "            bx = 0\n",
    "        if item_id in self.bi.keys():\n",
    "            bi = self.bi[item_id]\n",
    "        else:\n",
    "            bi = 0\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        score = self.global_avg + bx + bi + np.dot(p, q)\n",
    "        score = min(score, 100)\n",
    "        score = max(score, 0)\n",
    "        return score\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        loss += self.lambda_bx * np.linalg.norm(list(self.bx.values())) ** 2\n",
    "        loss += self.lambda_bi * np.linalg.norm(list(self.bi.values())) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    bx = self.bx[user_id]\n",
    "                    bi = self.bi[item_id]\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.bx[user_id] += lr * (error - self.lambda_bx * bx)\n",
    "                    self.bi[item_id] += lr * (error - self.lambda_bi * bi)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.9\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_model = SVD(baseline_data, factor=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [01:14<00:00, 265.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=27.93013527620241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [01:23<00:00, 238.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=27.79854495676883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [01:16<00:00, 258.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=27.777485680470978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [01:13<00:00, 270.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=27.807101586190697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [01:15<00:00, 264.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=27.856668487707616\n"
     ]
    }
   ],
   "source": [
    "SVD_model.train(5, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  16.57224154523797\n",
      "valid RMSE:  27.77936363893447\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_50factor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  16.062878369561574\n",
      "valid RMSE:  28.68405924929402\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/SVD_50factor.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "train_RMSE = RMSE(train_data, model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the attributes of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_attribute:\n",
    "    def __init__(self, baseline_data, similar_nodes, k = 3, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2, \n",
    "                 lambda_bx = 1e-2, lambda_bi = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            baseline_data: dict, baseline数据\n",
    "            similar_nodes: dict, 每个节点的相似节点\n",
    "            k: int, 使用的相似节点个数\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "            lambda_bx: float, 正则化参数\n",
    "            lambda_bi: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        self.lambda_bx = lambda_bx\n",
    "        self.lambda_bi = lambda_bi\n",
    "        # 用户与物品偏置\n",
    "        self.global_avg = baseline_data[\"global_avg\"]\n",
    "        self.bx = baseline_data[\"user_bias\"]\n",
    "        self.bi = baseline_data[\"item_bias\"]\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "        # 相似节点\n",
    "        self.similar_nodes = similar_nodes\n",
    "        self.k = k\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        if user_id in self.bx.keys():\n",
    "            bx = self.bx[user_id]\n",
    "        else:\n",
    "            bx = 0\n",
    "        if item_id in self.bi.keys():\n",
    "            bi = self.bi[item_id]\n",
    "        else:\n",
    "            bi = 0\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        # 直接得分由SVD模型得到\n",
    "        direct_score = self.global_avg + bx + bi + np.dot(p, q)\n",
    "        # indrect_score由相似节点得分平均得到\n",
    "        indirect_score, count = 0, 0\n",
    "        if item_id in self.similar_nodes:\n",
    "            for node_id in self.similar_nodes[item_id]:\n",
    "                temp_q = self.Q[:, node_id]\n",
    "                indirect_score += np.dot(p, temp_q)\n",
    "                count += 1\n",
    "                if count == self.k:\n",
    "                    break\n",
    "        if count == 0:\n",
    "            score = direct_score\n",
    "        else:\n",
    "            score = direct_score * 0.7 + (indirect_score / count) * 0.3\n",
    "        score = min(score, 100)\n",
    "        score = max(score, 0)\n",
    "        return score\n",
    "\n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        loss += self.lambda_bx * np.linalg.norm(list(self.bx.values())) ** 2\n",
    "        loss += self.lambda_bi * np.linalg.norm(list(self.bi.values())) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    bx = self.bx[user_id]\n",
    "                    bi = self.bi[item_id]\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.bx[user_id] += lr * (error - self.lambda_bx * bx)\n",
    "                    self.bi[item_id] += lr * (error - self.lambda_bi * bi)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(similar_nodes): 507172\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/similar_nodes.pkl\", \"rb\") as f:\n",
    "    similar_nodes = pickle.load(f)\n",
    "print(\"len(similar_nodes):\", len(similar_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_attribute_model = SVD_attribute(baseline_data, similar_nodes, k = 3, factor=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [01:40<00:00, 198.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=27.30137629835513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [01:32<00:00, 213.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=27.0941509565282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [01:42<00:00, 193.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=27.030544371750192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [01:36<00:00, 204.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=27.0330164292968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [01:40<00:00, 197.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=27.063990736766126\n"
     ]
    }
   ],
   "source": [
    "SVD_attribute_model.train(5, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  16.652290000697285\n",
      "valid RMSE:  26.983873535843795\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_attribute_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_attribute_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_attribute_50_3_73.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_attribute_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_basic:\n",
    "    def __init__(self, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        score = np.dot(p, q)\n",
    "        score = min(score, 100)\n",
    "        score = max(score, 0)\n",
    "        return score\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [00:55<00:00, 360.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=38.97872013984713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [00:51<00:00, 382.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=33.48464460046086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [00:51<00:00, 383.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=31.75150813051075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [00:51<00:00, 385.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=30.796652115769383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [00:53<00:00, 371.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=30.22794755420342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 19835/19835 [00:53<00:00, 369.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished: validate loss=29.886327662161055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 19835/19835 [00:52<00:00, 378.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished: validate loss=29.681855188338872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 19835/19835 [00:54<00:00, 363.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished: validate loss=29.55856717474021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 19835/19835 [00:51<00:00, 383.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished: validate loss=29.484571964242537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 19835/19835 [00:51<00:00, 383.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished: validate loss=29.440319012063522\n"
     ]
    }
   ],
   "source": [
    "SVD_basic_model = SVD_basic(factor=50)\n",
    "SVD_basic_model.train(10, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  17.82015769020438\n",
      "valid RMSE:  29.4390823320279\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_basic_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_basic_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_basic_50factor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_basic_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
