{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "- baseline estimator\n",
    "- collaborative filtering (user-based)\n",
    "- collaborative filtering (item-based)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    # 读取train.txt格式的数据，返回字典\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline().strip()\n",
    "            if not line:  # EOF\n",
    "                break\n",
    "            # 读取user_id和rate_num\n",
    "            user_id, rate_num = line.split('|')\n",
    "            rate_num = int(rate_num)\n",
    "            user_id = int(user_id)\n",
    "            # 读取用户的评分数据\n",
    "            rate_data = {}\n",
    "            for i in range(rate_num):\n",
    "                item_id, score = f.readline().strip().split()\n",
    "                item_id = int(item_id)\n",
    "                score = float(score)\n",
    "                rate_data[item_id] = score\n",
    "            # 保存该用户的数据\n",
    "            data[user_id] = rate_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data): 19835\n"
     ]
    }
   ],
   "source": [
    "train_path =\"data/train_data.txt\"\n",
    "train_data = read_data(train_path)\n",
    "print(\"len(train_data):\", len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### μ : overall mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_avg: 49.47062753677192\n"
     ]
    }
   ],
   "source": [
    "# 计算全局平均分\n",
    "def cal_global_avg(data):\n",
    "    sum_score = 0\n",
    "    sum_num = 0\n",
    "    for user_id, rate_data in data.items():\n",
    "        sum_score += sum(rate_data.values())\n",
    "        sum_num += len(rate_data)\n",
    "    return sum_score / sum_num\n",
    "\n",
    "global_avg = cal_global_avg(train_data)\n",
    "print(\"global_avg:\", global_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_x : rating deviation of user x (ave.rating of user x - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (547, 50.52937246322808)\n",
      "min_bias: (413, -49.47062753677192)\n",
      "average_bias: 20.37407831847788\n"
     ]
    }
   ],
   "source": [
    "# 统计每个用户的平均评分，用户偏差\n",
    "def cal_user_bias(data, average_score):\n",
    "    # 每个用户的平均评分\n",
    "    user_average_score = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        total_score = 0\n",
    "        for score in rate_data.values():\n",
    "            total_score += score\n",
    "        user_average_score[user_id] = total_score / len(rate_data)\n",
    "    # 每个用户与全局平均评分的偏差\n",
    "    user_bias = {}\n",
    "    for user_id, u_ave_score in user_average_score.items():\n",
    "        user_bias[user_id] = u_ave_score - average_score\n",
    "    # 最小偏差，最大偏差，平均偏差\n",
    "    max_bias = max(user_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(user_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in user_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(user_bias)\n",
    "    return user_average_score, user_bias, max_bias, min_bias, average_bias\n",
    "\n",
    "user_average_score, user_bias, max_bias, min_bias, average_bias = cal_user_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_i : rating deviation of item i (ave.rating of item i - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (210761, 50.52937246322808)\n",
      "min_bias: (211658, -49.47062753677192)\n",
      "average_bias: -5.654850049099554\n"
     ]
    }
   ],
   "source": [
    "# 统计每个物品的平均评分，物品偏差\n",
    "def cal_item_bias(data, average_score):\n",
    "    # 统计物品得分\n",
    "    item_scores = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            if item_id in item_scores:\n",
    "                item_scores[item_id].append(score)\n",
    "            else:\n",
    "                item_scores[item_id] = [score]\n",
    "    # 计算物品平均得分\n",
    "    item_average_score = {}\n",
    "    for item_id, scores in item_scores.items():\n",
    "        item_average_score[item_id] = sum(scores) / len(scores)\n",
    "    # 计算物品偏差\n",
    "    item_bias = {}\n",
    "    for item_id, i_ave_score in item_average_score.items():\n",
    "        item_bias[item_id] = i_ave_score - average_score\n",
    "    # 最大偏差，最小偏差，平均偏差\n",
    "    max_bias = max(item_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(item_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in item_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(item_bias)\n",
    "    return item_average_score, item_bias, max_bias, min_bias, average_bias\n",
    "\n",
    "item_average_score, item_bias, max_bias, min_bias, average_bias = cal_item_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_estimator: 3993264\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# b_xi = μ + b_x + b_i\n",
    "baseline_estimator = {}  \n",
    "for user_id, rate_data in train_data.items():\n",
    "    for item_id, score in rate_data.items():\n",
    "        baseline_estimator[(user_id, item_id)] = global_avg + user_bias[user_id] + item_bias[item_id]\n",
    "\n",
    "with open(\"models/baseline_estimator.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_estimator, f)\n",
    "\n",
    "print(\"baseline_estimator:\", len(baseline_estimator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collaborative filtering (user-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pearson(x, y, x_ave, y_ave):\n",
    "    \"\"\"\n",
    "    calculate pearson correlation coefficient\n",
    "    Args:\n",
    "        x: the score list of x (user1)\n",
    "        y: the score list of y (user2)\n",
    "        x_ave: the average score of x (user1)\n",
    "        y_ave: the average score of y (user2)\n",
    "    Returns:\n",
    "        sim(x, y): the pearson correlation coefficient between x and y\n",
    "    \"\"\"\n",
    "    # 找到两个用户共同评分的物品\n",
    "    shared_items = set(x.keys()) & set(y.keys())\n",
    "\n",
    "    # 如果没有共同元素，返回无穷\n",
    "    if not shared_items:\n",
    "        return math.inf\n",
    "\n",
    "    # 计算pearson相关系数\n",
    "    sim, sum1, sum2 = 0, 0, 0\n",
    "    for item in shared_items:\n",
    "        temp1 = x[item] - x_ave\n",
    "        temp2 = y[item] - y_ave\n",
    "        # 为了避免分母为0的情况，对将打分值做一个微调\n",
    "        if temp1 == 0:\n",
    "            temp1 = 0.1\n",
    "        if temp2 == 0:\n",
    "            temp2 = 0.1\n",
    "        sim += temp1 * temp2  # 分子\n",
    "        # 计算分母\n",
    "        sum1 += temp1**2\n",
    "        sum2 += temp2**2\n",
    "    sim = sim / ((sum1**0.5) * (sum2**0.5))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 计算两两用户的相似度\n",
    "def cal_similarity(train_set, user_average_score):\n",
    "    \"\"\"\n",
    "    calculate the similarity between users\n",
    "    Args:\n",
    "        train_set: the train data\n",
    "    Returns:\n",
    "        similarity: the similarity matrix\n",
    "    \"\"\"\n",
    "    similarity = {key:{} for key in train_set.keys()}\n",
    "    for i, user1 in tqdm(enumerate(train_set.keys()), desc=\"Outer Loop\"):\n",
    "        for j, user2 in enumerate(list(train_set.keys())[i+1:], start=i+1):\n",
    "            pearson_sim = pearson(train_set[user1], train_set[user2], user_average_score[user1], user_average_score[user2])\n",
    "            similarity[user1][user2] = pearson_sim\n",
    "            similarity[user2][user1] = pearson_sim\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cal_similarity(train_data, user_average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每个用户找最相似的k个用户\n",
    "def find_k_nearest(similarity, k):\n",
    "    \"\"\"\n",
    "    find the k nearest neighbors for each user\n",
    "    Args:\n",
    "        similarity: the similarity matrix\n",
    "        k: the number of nearest neighbors\n",
    "    Returns:\n",
    "        nearest: the k nearest neighbors for each user\n",
    "    \"\"\"\n",
    "    nearest_k = {}\n",
    "    for user in similarity.keys():\n",
    "        nearest_k[user] = heapq.nlargest(k, similarity[user], key=similarity[user].get)\n",
    "    return nearest_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': {'b': 2, 'c': 1.5}, 'b': {'b': 2.2, 'c': 1.5}, 'c': {'b': 5.2, 'a': 4.1}}\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "test={'a':{'a':1,'b':2,'c':1.5},'b':{'a':1.2,'b':2.2,'c':1.5},'c':{'a':4.1,'b':5.2,'c':1.5}}\n",
    "\n",
    "for item in nearest.keys():\n",
    "    for item2 in nearest[item].keys():\n",
    "        if nearest[item][item2]>1:\n",
    "            nearest[item][item2]=0\n",
    "\n",
    "def find_k_nearest(train_set, k):\n",
    "    top_k_keys = {}\n",
    "    \n",
    "    for outer_key, inner_map in train_set.items():\n",
    "        # 使用heapq.nlargest找到前k个值最大的键\n",
    "        top_k = heapq.nlargest(k, inner_map.items(), key=lambda item: item[1])\n",
    "        # 将结果存储在top_k_keys中，只保留键\n",
    "        top_k_keys[outer_key] = {key:value for key, value in top_k}\n",
    "    \n",
    "    return top_k_keys\n",
    "\n",
    "k_test=find_k_nearest(test,2)\n",
    "k_nearest=find_k_nearest(nearest,10)\n",
    "print(k_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path =\"data/nearest.txt\"\n",
    "def write_to_file(file_path,data):\n",
    "    with open(file_path,\"w\") as f:\n",
    "        for item1 in data.keys():\n",
    "            f.write(str(item1))\n",
    "            for item2 in data[item1].keys():\n",
    "                f.write(str(item2)+\" \"+str(data[item1][item2])+'\\n')\n",
    "write_to_file(result_path,nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2': 1.0, '3': 1.0, '9': 1.0, '20': 1.0, '25': 1.0, '26': 1.0, '34': 1.0, '38': 1.0, '46': 1.0, '47': 1.0}\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(similar)\n\u001b[1;32m---> 14\u001b[0m predict\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m\n\u001b[0;32m     15\u001b[0m predict\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39muser_average[user]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predict\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m100\u001b[39m:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "RMSE=0\n",
    "num=0\n",
    "for user in valid_list.keys():\n",
    "    similar=k_nearest[user]\n",
    "    for item in valid_list[user].keys():\n",
    "        sum=0\n",
    "        predict=0\n",
    "        for sim_user in similar.keys():\n",
    "            if item in user_list[sim_user].keys():\n",
    "                predict+=similar[sim_user]*user_list[sim_user][item]\n",
    "                sum+=similar[sim_user]\n",
    "        if sum==0:\n",
    "            print(similar)\n",
    "        predict/=sum\n",
    "        predict+=user_average[user]\n",
    "        if predict>100:\n",
    "            predict=100\n",
    "        if predict<0:\n",
    "            predict=0\n",
    "        RMSE+=(predict-valid_list[user][item])**2\n",
    "        num+=1\n",
    "RMSE=(RMSE/num)**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list={}\n",
    "test_path=\"data/test.txt\"\n",
    "\n",
    "with open(test_path,\"r\") as f:\n",
    "    while True:\n",
    "        data=f.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        data=data.split('|')\n",
    "        user_id,rate_nums=data[0],data[1]\n",
    "        user_rate={}\n",
    "        for i in range(int(rate_nums)):\n",
    "            rate=f.readline()\n",
    "            user_rate[rate[0]]=0\n",
    "        test_list[user_id]=user_rate\n",
    "\n",
    "for user in test_list.keys():\n",
    "    similar=k_nearest[user]\n",
    "    for item in test_list[user].keys():\n",
    "        sum=0\n",
    "        for sim_user in similar.keys():\n",
    "            if item in user_list[sim_user].keys():\n",
    "                test_list[user][item]+=similar[sim_user]*user_list[sim_user][item]\n",
    "                sum+=similar[sim_user]\n",
    "        test_list[user][item]/=sum\n",
    "        test_list[user][item]+=user_average[user]\n",
    "        if test_list[user][item]>100:\n",
    "            test_list[user][item]=100\n",
    "        if test_list[user][item]<0:\n",
    "            test_list[user][item]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data[user]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      6\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(data[user][item])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m write_to_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/result.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mtest_list\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_list' is not defined"
     ]
    }
   ],
   "source": [
    "def write_to_file(file_path,data):\n",
    "    with open(file_path,\"w\") as f:\n",
    "        for user in data.keys():\n",
    "            f.write(str(user)+\"|\"+str(len(data[user]))+'\\n')\n",
    "            for item in data[user].keys():\n",
    "                f.write(str(item)+\" \"+str(data[user][item])+'\\n')\n",
    "\n",
    "write_to_file(\"data/result.txt\",test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collaborative filtering (item-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set={}\n",
    "valid_set={}\n",
    "\n",
    "train_path =\"/content/drive/MyDrive/RecSys/data/train_set.txt\"\n",
    "valid_path =\"/content/drive/MyDrive/RecSys/data/valid_set.txt\"\n",
    "\n",
    "\n",
    "with open(train_path, \"r\") as f:\n",
    "    while True:\n",
    "        data=f.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        data=data.split('|')\n",
    "        user_id,rate_nums= data[0],data[1]\n",
    "        for i in range(int(rate_nums)):\n",
    "            rate=f.readline()\n",
    "            rate=rate.split()\n",
    "            if rate[0] not in train_set:\n",
    "              train_set[rate[0]]={}\n",
    "            train_set[rate[0]][user_id]=float(rate[1])\n",
    "\n",
    "with open(valid_path,\"r\") as f:\n",
    "    while True:\n",
    "        data=f.readline()\n",
    "        if not data:\n",
    "            break;\n",
    "        data=data.split('|')\n",
    "        user_id,rate_nums=data[0],data[1]\n",
    "        for i in range(int(rate_nums)):\n",
    "            rate=f.readline()\n",
    "            rate=rate.split()\n",
    "            if rate[0] not in valid_set:\n",
    "              valid_set[rate[0]]={}\n",
    "            valid_set[rate[0]][user_id]=float(rate[1])\n",
    "# 输出user_list的大小\n",
    "print(len(train_set))\n",
    "# 输出valid_list的大小\n",
    "print(len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_average = {}\n",
    "# 计算每个Item的平均打分\n",
    "for item in train_set.keys():\n",
    "    sum=0\n",
    "    for user in train_set[item].keys():\n",
    "        sum+=train_set[item][user]\n",
    "    item_average[item]=sum/len(train_set[item])\n",
    "    for user in train_set[item].keys():\n",
    "      train_set[item][user]-=user_average[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复用user的pearson相关系数，传参的时候传item即可\n",
    "def pearson(user1,user2):\n",
    "    \"\"\"\n",
    "    计算pearson correlation coefficient\n",
    "    Args:\n",
    "        user1:用户1的打分列表\n",
    "        user2:用户2的打分列表\n",
    "    Returns:\n",
    "        pearson相关系数\n",
    "    \"\"\"\n",
    "    # 获得共有的item\n",
    "    shared=set(user1.keys()) & set(user2.keys())\n",
    "\n",
    "    # 如果没有共同元素，返回无穷\n",
    "    if not shared:\n",
    "        return math.inf\n",
    "\n",
    "    # 计算pearson相关系数\n",
    "    sim=0\n",
    "    sum1=0\n",
    "    sum2=0\n",
    "    for item in shared:\n",
    "        temp1=user1[item]\n",
    "        temp2=user2[item]\n",
    "        # 为了避免分母为0的情况，对将打分值做一个微调\n",
    "        if temp1==0:\n",
    "            temp1=0.1\n",
    "        if temp2==0:\n",
    "            temp2=0.1\n",
    "        sim+=temp1*temp2\n",
    "        sum1+=temp1**2\n",
    "        sum2+=temp2**2\n",
    "    sim=sim/((sum1**0.5)*(sum2**0.5))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "# 计算两两物品的相似度\n",
    "def cal_similarity(train_set):\n",
    "    \"\"\"\n",
    "    计算每个用户其最相似的k个用户\n",
    "\n",
    "    Args:\n",
    "        train_set:训练集\n",
    "\n",
    "    Returns:\n",
    "        similarity:相似度矩阵\n",
    "    \"\"\"\n",
    "    similarity={key:{} for key in train_set.keys()}\n",
    "    for i, user1 in tqdm(enumerate(train_set.keys()), desc=\"Outer Loop\"):\n",
    "        for j, user2 in enumerate(list(train_set.keys())[i+1:], start=i+1):\n",
    "            pearson_sim = pearson(train_set[user1], train_set[user2])\n",
    "            similarity[user1][user2] = pearson_sim\n",
    "            similarity[user2][user1] = pearson_sim\n",
    "    return similarity\n",
    "similarity=cal_similarity(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest(train_set, k):\n",
    "    top_k_keys = {}\n",
    "\n",
    "    for outer_key, inner_map in train_set.items():\n",
    "        # 使用heapq.nlargest找到前k个值最大的键\n",
    "        top_k = heapq.nlargest(k, inner_map.items(), key=lambda item: item[1])\n",
    "        # 将结果存储在top_k_keys中，只保留键\n",
    "        top_k_keys[outer_key] = {key:value for key, value in top_k}\n",
    "\n",
    "    return top_k_keys\n",
    "\n",
    "k_nearest=find_k_nearest(nearest,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
